{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75776460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e09d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 11\n",
    "environment_columns = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.zeros((environment_rows, environment_columns, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D numpy array to hold the rewards for each state.\n",
    "# The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
    "rewards = np.full((environment_rows, environment_columns), -100.)\n",
    "rewards[0, 5] = 100.  # set the reward for the packaging area (i.e., the goal) to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b91cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define aisle locations (i.e., white squares) for rows 1 through 9\n",
    "aisles = {}  # store locations in a dictionary\n",
    "aisles[1] = [i for i in range(1, 10)]\n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 8)]\n",
    "aisles[3].append(9)\n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range(11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the rewards for all aisle locations (i.e., white squares)\n",
    "for row_index in range(1, 10):\n",
    "    for column_index in aisles[row_index]:\n",
    "        rewards[row_index, column_index] = -1.\n",
    "\n",
    "# print rewards matrix\n",
    "for row in rewards:\n",
    "    print(row)\n",
    "\n",
    "    #Training the model\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(current_row_index, current_column_index):\n",
    "        # if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
    "        if rewards[current_row_index, current_column_index] == -1.:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location():\n",
    "        # get a random row and column index\n",
    "        current_row_index = np.random.randint(environment_rows)\n",
    "        current_column_index = np.random.randint(environment_columns)\n",
    "        # continue choosing random row and column indexes until a non-terminal state is identified\n",
    "        # (i.e., until the chosen state is a 'white square').\n",
    "        while is_terminal_state(current_row_index, current_column_index):\n",
    "            current_row_index = np.random.randint(environment_rows)\n",
    "            current_column_index = np.random.randint(environment_columns)\n",
    "        return current_row_index, current_column_index\n",
    "\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "        # if a randomly chosen value between 0 and 1 is less than epsilon,\n",
    "        # then choose the most promising value from the Q-table for this state.\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(q_values[current_row_index, current_column_index])\n",
    "        else:  # choose a random action\n",
    "            return np.random.randint(4)\n",
    "\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(current_row_index, current_column_index, action_index):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "            new_column_index += 1\n",
    "        elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "            new_row_index += 1\n",
    "        elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "\n",
    "    # Define a function that will get the shortest path between any location within the warehouse that\n",
    "    # the robot is allowed to travel and the item packaging location.\n",
    "    def get_shortest_path(start_row_index, start_column_index):\n",
    "        # return immediately if this is an invalid starting location\n",
    "        if is_terminal_state(start_row_index, start_column_index):\n",
    "            return []\n",
    "        else:  # if this is a 'legal' starting location\n",
    "            current_row_index, current_column_index = start_row_index, start_column_index\n",
    "            shortest_path = []\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "            # continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "            while not is_terminal_state(current_row_index, current_column_index):\n",
    "                # get the best action to take\n",
    "                action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "                # move to the next location on the path, and add the new location to the list\n",
    "                current_row_index, current_column_index = get_next_location(current_row_index, current_column_index,\n",
    "                                                                            action_index)\n",
    "                shortest_path.append([current_row_index, current_column_index])\n",
    "            return shortest_path\n",
    "\n",
    "        # define training parameters\n",
    "        epsilon = 0.9  # the percentage of time when we should take the best action (instead of a random action)\n",
    "        discount_factor = 0.9  # discount factor for future rewards\n",
    "        learning_rate = 0.9  # the rate at which the AI agent should learn\n",
    "\n",
    "        # run through 1000 training episodes\n",
    "        for episode in range(1000):\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = get_starting_location()\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            # (i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "            while not is_terminal_state(row_index, column_index):\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                # receive the reward for moving to the new state, and calculate the temporal difference\n",
    "                reward = rewards[row_index, column_index]\n",
    "                old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "                q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "        print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9449cb44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_shortest_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# display a few shortest paths\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_shortest_path\u001b[49m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m))  \u001b[38;5;66;03m# starting at row 3, column 9\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_shortest_path(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# starting at row 5, column 0\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_shortest_path(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m5\u001b[39m))  \u001b[38;5;66;03m# starting at row 9, column 5\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_shortest_path' is not defined"
     ]
    }
   ],
   "source": [
    "  # display a few shortest paths\n",
    "print(get_shortest_path(3, 9))  # starting at row 3, column 9\n",
    "print(get_shortest_path(5, 0))  # starting at row 5, column 0\n",
    "print(get_shortest_path(9, 5))  # starting at row 9, column 5\n",
    "\n",
    "        # display an example of reversed shortest path\n",
    "path = get_shortest_path(5, 2)  # go to row 5, column 2\n",
    "path.reverse()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566077a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
